{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_pca.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml58PJ9UDwRk"
      },
      "source": [
        "**Principal Component Analysis**\n",
        "\n",
        "You will implement dimensionality reduction with PCA.  \n",
        "\n",
        "1). Read iris_dataset.csv (4 features, hence 4 PCs)\n",
        "\n",
        "2). Find the principal components\n",
        "\n",
        "3). Recontruct the dataset (X_hat)\n",
        "\n",
        "4). Determine the accuracy of X_hat for 1 PC and 4 PCs using LDA classifier (provided below)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SifztemxaVOw",
        "outputId": "5de1c72a-0010-45cf-92eb-d357c1b8a6d1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSYk7yTsafks"
      },
      "source": [
        "data_path = \"/content/drive/MyDrive/Data/PCA/iris_dataset.csv\""
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3DA-QxT0O6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ba89d3-14a0-4957-c58d-6283395a510b"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import linalg as LA\n",
        "\n",
        "\n",
        "# Load data - 150 observations, 4 features, 3 classes, \n",
        "df = pd.read_csv(data_path, header=None)\n",
        "print(df.describe())\n",
        "data = df.values"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                0           1           2           3           4\n",
            "count  150.000000  150.000000  150.000000  150.000000  150.000000\n",
            "mean     5.843333    3.057333    3.758000    1.199333    2.000000\n",
            "std      0.828066    0.435866    1.765298    0.762238    0.819232\n",
            "min      4.300000    2.000000    1.000000    0.100000    1.000000\n",
            "25%      5.100000    2.800000    1.600000    0.300000    1.000000\n",
            "50%      5.800000    3.000000    4.350000    1.300000    2.000000\n",
            "75%      6.400000    3.300000    5.100000    1.800000    3.000000\n",
            "max      7.900000    4.400000    6.900000    2.500000    3.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J_I64r12CK1"
      },
      "source": [
        "## Setup\n",
        "\n",
        "# Shuffle data randomly\n",
        "shuffled_data = data;\n",
        "np.random.shuffle(shuffled_data)\n",
        "X = shuffled_data[:,0:4]  # 150x4\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3llQ6-RP00N"
      },
      "source": [
        "def evaluate_performance(Xhat, Num_PC, recon_error):\n",
        "  \n",
        "  ##################################\n",
        "  # Evaluate performance using LDA #\n",
        "  ##################################\n",
        "  \n",
        "  from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "  from sklearn.model_selection import cross_val_score\n",
        "\n",
        "  no_dim = Num_PC  # number of dimensions\n",
        "  #X_train = X[:,0:no_dim]           # original dataset\n",
        "  X_train = Xhat[:,0:Num_PC]        # dimensionally reduced dataset\n",
        "  y_train = data[:,4]\n",
        "\n",
        "  model_mean_scores = []\n",
        "  model = LinearDiscriminantAnalysis().fit(X_train, y_train)\n",
        "  scores = cross_val_score(model, X_train, y_train, cv=10)\n",
        "  model_mean_scores.append(np.mean(scores))\n",
        "\n",
        "  # Reconstruction error\n",
        "  print('Reconstruction error = {0:0.6f} with {1:1d} PCs, average accuracy = {2:0.4f}'\n",
        "     .format(recon_error, Num_PC, model_mean_scores[0]))"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-L8WvJIAKeY"
      },
      "source": [
        "def create_pc(X, Num_PC):\n",
        "  '''\n",
        "    Inputs: \n",
        "      X : original data set, size 150x4\n",
        "      Num_PC : number of PC's to be used to recover Xhat\n",
        "    Outputs:\n",
        "      PC : principal components, size 150x4\n",
        "      Xhat : reconstructed data set, size 150x4\n",
        "      recon_error : reconstruction error\n",
        "        (l2norm(X) - l2norm(Xhat)) / (l2norm(X))\n",
        "  '''\n",
        "\n",
        "  # Create zero mean dataset\n",
        "  # Find the covariance matrix\n",
        "  # Apply eigen-decomposition\n",
        "  #   Hint: use LA.eig\n",
        "  # Compute principal components\n",
        "\n",
        "  # Reconstruct dataset (Xhat)\n",
        "  # Compute reconstruction error\n",
        "  \n",
        "  #### Your code goes here ...\n",
        "\n",
        "  #Finding Mean Vector\n",
        "  u = [np.mean(X[:,0]), np.mean(X[:,1]), np.mean(X[:,2]), np.mean(X[:,3])]\n",
        "  #Subtracting mean vector from data\n",
        "  XM = np.subtract(X,u)\n",
        "  #finding Covariance matrix\n",
        "  cov = np.cov(XM.T)\n",
        "  #Finding EigenValues and EigenVectors\n",
        "  w, Phi = LA.eig(cov)\n",
        "  index = np.argsort(w)[::-1]\n",
        "  #Phi = matrix of eigenvectors\n",
        "  w = w[index]\n",
        "  Phi = Phi[:,index]\n",
        "  \n",
        "  PC = Phi[:,0:Num_PC]\n",
        "  #Finding Y = XM * PC[i]\n",
        "  Y = np.dot(XM,PC)\n",
        "  #Xhat = Y*PC(transpose)*meean_vector \n",
        "  Xhat = np.add(np.dot(Y,PC.T),u)\n",
        "  recon_error = (np. linalg.norm(X)-np.linalg.norm(Xhat))/np.linalg.norm(X)\n",
        "\n",
        "  return Phi, Xhat, recon_error"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87Rs58BJbMvR"
      },
      "source": [
        "PC, X_hat, recon_err = create_pc(X,1)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wh_U_JnLhjy",
        "outputId": "8d253f6b-74d2-4224-89e4-c943ea9e7b57"
      },
      "source": [
        "PC, X_hat, recon_err = create_pc(X,1)\n",
        "evaluate_performance(X_hat, 1, recon_err)\n",
        "\n",
        "PC, X_hat, recon_err = create_pc(X,4)\n",
        "evaluate_performance(X_hat, 4, recon_err)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstruction error = 0.002696 with 1 PCs, average accuracy = 0.9400\n",
            "Reconstruction error = 0.000000 with 4 PCs, average accuracy = 0.9800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa9a4-yNImGL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}